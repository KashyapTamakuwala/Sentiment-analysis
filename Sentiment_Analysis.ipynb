{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c811069f",
      "metadata": {
        "id": "c811069f",
        "outputId": "a0466431-c959-462c-ea6f-8715cd3cb972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /home/terificknight/anaconda3/lib/python3.8/site-packages (2.1.0)\n",
            "Requirement already satisfied: tqdm in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.62.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: scipy in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scikit-learn in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.24.2)\n",
            "Requirement already satisfied: torchvision in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.11.1)\n",
            "Requirement already satisfied: nltk in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (3.6.5)\n",
            "Requirement already satisfied: numpy in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.21.4)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.10.0)\n",
            "Requirement already satisfied: sentencepiece in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: huggingface-hub in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.1.0)\n",
            "Requirement already satisfied: typing_extensions in /home/terificknight/anaconda3/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.8.3)\n",
            "Requirement already satisfied: sacremoses in /home/terificknight/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /home/terificknight/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /home/terificknight/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /home/terificknight/anaconda3/lib/python3.8/site-packages (from nltk->sentence-transformers) (8.0.1)\n",
            "Requirement already satisfied: joblib in /home/terificknight/anaconda3/lib/python3.8/site-packages (from nltk->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: six in /home/terificknight/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/terificknight/anaconda3/lib/python3.8/site-packages (from torchvision->sentence-transformers) (8.3.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf7c078",
      "metadata": {
        "id": "abf7c078",
        "outputId": "988ba76c-774c-4479-e1e4-ac9b7b4924d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/terificknight/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/terificknight/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/terificknight/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/terificknight/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "2021-11-12 09:31:10.810529: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2021-11-12 09:31:10.810622: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gensim \n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b83135",
      "metadata": {
        "id": "81b83135",
        "outputId": "5db2f911-b74c-41ab-cc9a-8ae4ee64747f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/terificknight/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: Could not cast to float64, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "## Reading data file and converting them into csv files.\n",
        "\n",
        "#Reading Files.\n",
        "file1 = open('data2/train.dat', 'r')\n",
        "Lines = file1.readlines()\n",
        "\n",
        "file2 = open('data2/test.dat', 'r')\n",
        "Lines2 = file2.readlines()\n",
        "\n",
        "#storing data in a list\n",
        "train_data=[]\n",
        "\n",
        "for i in Lines:\n",
        "    x=int(i[0:2])\n",
        "    train_data.append([x,i[3:-1].replace(\"<br />\",\"\")])\n",
        "\n",
        "\n",
        "test_text=[]\n",
        "for i in Lines2:\n",
        "    test_text.append(i[:-1])\n",
        "    #print(i)\n",
        "\n",
        "# Converting list to dataframe and saving into a csv file.\n",
        "train_data=pd.DataFrame(train_data,columns=[\"labels\",\"Reviews\"],dtype=float)\n",
        "train_data.to_csv(\"data2/train_data.csv\",index=False)\n",
        "test_data=pd.DataFrame(test_text,columns=[\"Reviews\"])\n",
        "test_data.to_csv(\"data2/test_data.csv\",index=False)\n",
        "\n",
        "del train_data\n",
        "del test_data\n",
        "del file1\n",
        "del file2\n",
        "del Lines\n",
        "del Lines2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e44f56",
      "metadata": {
        "id": "01e44f56"
      },
      "outputs": [],
      "source": [
        "train_data=pd.read_csv(\"data2/train_data.csv\")\n",
        "test_data=pd.read_csv(\"data2/test_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f2020d7",
      "metadata": {
        "id": "9f2020d7"
      },
      "outputs": [],
      "source": [
        "def mean(z): # used for BERT (word version) and Word2Vec\n",
        "    return sum(itertools.chain(z))/len(z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c98b1a22",
      "metadata": {
        "id": "c98b1a22"
      },
      "outputs": [],
      "source": [
        "# def NormalizeWithPOS(text):\n",
        "#     # Lemmatization & Stemming according to POS tagging\n",
        "\n",
        "#     word_list = word_tokenize(text)\n",
        "#     rev = []\n",
        "#     lemmatizer = WordNetLemmatizer() \n",
        "#     stemmer = PorterStemmer() \n",
        "#     for word, tag in pos_tag(word_list):\n",
        "#         if tag.startswith('J'):\n",
        "#             w = lemmatizer.lemmatize(word, pos='a')\n",
        "#         elif tag.startswith('V'):\n",
        "#             w = lemmatizer.lemmatize(word, pos='v')\n",
        "#         elif tag.startswith('N'):\n",
        "#             w = lemmatizer.lemmatize(word, pos='n')\n",
        "#         elif tag.startswith('R'):\n",
        "#             w = lemmatizer.lemmatize(word, pos='r')\n",
        "#         else:\n",
        "#             w = word\n",
        "#         w = stemmer.stem(w)\n",
        "#         rev.append(w)\n",
        "#     review = ' '.join(rev)\n",
        "#     return review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e13dfcc",
      "metadata": {
        "id": "6e13dfcc"
      },
      "outputs": [],
      "source": [
        "def cleanText(text):\n",
        "    \n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "\n",
        "    \n",
        "    # Replace punctuations with space\n",
        "    # save ! ? . for end of the sentence detection [,/():;']\n",
        "    filters='\"#$%&*+<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "    text = re.sub(r'\\!+', '!', text)\n",
        "    text = re.sub(r'\\?+', '?', text)\n",
        "\n",
        "    translate_dict = dict((i, \" \") for i in filters)\n",
        "    translate_map = str.maketrans(translate_dict)\n",
        "    text = text.translate(translate_map)\n",
        "    \n",
        "    \n",
        "    text = re.sub(r'\\( *\\)', ' ', text)\n",
        "\n",
        "    # Replace multiple space with one space\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    \n",
        "    text = ''.join(text)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4aee01",
      "metadata": {
        "id": "ce4aee01"
      },
      "outputs": [],
      "source": [
        "def embeddToBERT(text):\n",
        "    sentences = re.split('!|\\?|\\.',text)\n",
        "    sentences = list(filter(None, sentences)) \n",
        "    \n",
        "    ## encoding the sentence\n",
        "    result = bert_transformers.encode(sentences)\n",
        "    #sys.stdout.write('\\r'+\"in\")\n",
        "    feature = [mean(x) for x in zip(*result)]\n",
        "  \n",
        "    return np.asarray(feature).reshape((768,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ffa45be",
      "metadata": {
        "id": "9ffa45be"
      },
      "source": [
        "## Cleaning Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a95a393e",
      "metadata": {
        "id": "a95a393e"
      },
      "outputs": [],
      "source": [
        "embedding = 'BERT'\n",
        "# for Word2Vec with stop words\n",
        "train_data['clean_text'] = train_data['Reviews'].apply(cleanText)\n",
        "test_data['clean_text'] = test_data['Reviews'].apply(cleanText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e184a10c",
      "metadata": {
        "id": "e184a10c",
        "outputId": "56c49dca-ed84-47c6-8964-a668bcbb09ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>Although a film with Bruce Willis is always wo...</td>\n",
              "      <td>Although a film with Bruce Willis is always wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>This movie was slower then Molasses in January...</td>\n",
              "      <td>This movie was slower then Molasses in January...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>Interesting film about an actual event that to...</td>\n",
              "      <td>Interesting film about an actual event that to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>It's painfully obvious that the people who mad...</td>\n",
              "      <td>It's painfully obvious that the people who mad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>This movie really is a mixed bag. On the one h...</td>\n",
              "      <td>This movie really is a mixed bag. On the one h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   labels                                            Reviews  \\\n",
              "0    -1.0  Although a film with Bruce Willis is always wo...   \n",
              "1    -1.0  This movie was slower then Molasses in January...   \n",
              "2    -1.0  Interesting film about an actual event that to...   \n",
              "3    -1.0  It's painfully obvious that the people who mad...   \n",
              "4     1.0  This movie really is a mixed bag. On the one h...   \n",
              "\n",
              "                                          clean_text  \n",
              "0  Although a film with Bruce Willis is always wo...  \n",
              "1  This movie was slower then Molasses in January...  \n",
              "2  Interesting film about an actual event that to...  \n",
              "3  It's painfully obvious that the people who mad...  \n",
              "4  This movie really is a mixed bag. On the one h...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb54cae",
      "metadata": {
        "id": "8fb54cae"
      },
      "source": [
        "## Split data into Traning and Validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b196cf2",
      "metadata": {
        "id": "4b196cf2",
        "outputId": "c9b3e560-1a73-4575-9557-3e4a7df7bf11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20227    Please...if anybody gets the chance to read th...\n",
              "9945     This film is absolutely horrific. One of the w...\n",
              "12379    The Muppet movie is an instant classic. I reme...\n",
              "3639     Ray Liotta and Tom Hulce shine in this sterlin...\n",
              "14592    Finally, Timon and Pumbaa in their own film......\n",
              "Name: clean_text, dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_data['clean_text'], train_data['labels'], test_size=0.33, random_state=321)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62d0805",
      "metadata": {
        "id": "c62d0805"
      },
      "source": [
        "## Bert Transformer for Converting Text to Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6660c0d1",
      "metadata": {
        "id": "6660c0d1"
      },
      "outputs": [],
      "source": [
        "bert_transformers = SentenceTransformer('bert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def7af68",
      "metadata": {
        "id": "def7af68"
      },
      "source": [
        "## Converting Training dataset Text to vectors using Bert transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee14f8c",
      "metadata": {
        "id": "1ee14f8c",
        "outputId": "c424ae1b-c511-47ef-dec5-6320a0ceb777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16749"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "#bert_versio= 'SENTENCE'\n",
        "c=0\n",
        "bert_sentence_training_features=[]\n",
        "for i in X_train:\n",
        "    sys.stdout.write('\\r'+str(c))\n",
        "    bert_sentence_training_features.append(embeddToBERT(i))\n",
        "    c=c+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d26576d",
      "metadata": {
        "id": "4d26576d",
        "outputId": "948e2c30-5f53-480f-b9e6-2d6052b5cc3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(16750, 768)\n"
          ]
        }
      ],
      "source": [
        "feature = [x.T for x in bert_sentence_training_features]\n",
        "bert_sentence_training_features = np.asarray(feature).reshape(len(X_train),768)\n",
        "\n",
        "print(bert_sentence_training_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7a7226",
      "metadata": {
        "id": "6b7a7226"
      },
      "outputs": [],
      "source": [
        "## saving into csv\n",
        "#np.savetxt('data/embedding_data_final3.csv', bert_sentence_training_features, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98a219d4",
      "metadata": {
        "id": "98a219d4"
      },
      "source": [
        "## Converting Validation dataset Text to vectors using Bert transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23032c0d",
      "metadata": {
        "id": "23032c0d",
        "outputId": "06e56eb3-c1da-4b68-bf06-ec8248a80941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8249"
          ]
        }
      ],
      "source": [
        "# bert_sentence_test_features = X_test.apply(embeddToBERT)\n",
        "#bert_version = 'SENTENCE'\n",
        "c=0\n",
        "bert_sentence_test_features=[]\n",
        "for i in X_test:\n",
        "    sys.stdout.write('\\r'+str(c))\n",
        "    bert_sentence_test_features.append(embeddToBERT(i))\n",
        "    c=c+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5e34b3d",
      "metadata": {
        "id": "a5e34b3d",
        "outputId": "a241d73d-b22e-41ef-c323-ab3a487e9153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8250, 768)\n"
          ]
        }
      ],
      "source": [
        "feature = [x.T for x in bert_sentence_test_features]\n",
        "bert_sentence_test_features = np.asarray(feature).reshape(len(X_test),768)\n",
        "print(bert_sentence_test_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65af10dc",
      "metadata": {
        "id": "65af10dc"
      },
      "source": [
        "## Initialize Svm model with Linear Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c4a5df",
      "metadata": {
        "id": "75c4a5df"
      },
      "outputs": [],
      "source": [
        "model = SVC(kernel ='linear', C = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9805b458",
      "metadata": {
        "id": "9805b458"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e306a04",
      "metadata": {
        "id": "4e306a04",
        "outputId": "9e21119a-9344-4833-9e81-3c02d2a245d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SVC(C=1, kernel='linear')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(bert_sentence_training_features, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "147a858a",
      "metadata": {
        "id": "147a858a"
      },
      "source": [
        "## Evaluation of model on validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63008890",
      "metadata": {
        "id": "63008890",
        "outputId": "20c28b9d-7da5-46ae-d55b-e3079766be10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.90      0.91      0.90      4124\n",
            "         1.0       0.91      0.90      0.90      4126\n",
            "\n",
            "    accuracy                           0.90      8250\n",
            "   macro avg       0.90      0.90      0.90      8250\n",
            "weighted avg       0.90      0.90      0.90      8250\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "y_pred_bert_words_svm = model.predict(bert_sentence_test_features)\n",
        "\n",
        "#y_prob_bert_words_svm = model.decision_function(bert_word_test_features)\n",
        "\n",
        "print(classification_report(y_test,y_pred_bert_words_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39eaaf73",
      "metadata": {
        "id": "39eaaf73"
      },
      "source": [
        "## Converting Test dataset to vectors using Bert Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2848f720",
      "metadata": {
        "id": "2848f720",
        "outputId": "3df76361-1e9c-477c-df51-647ea94099f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24999"
          ]
        }
      ],
      "source": [
        "#bert_version = 'SENTENCE'\n",
        "c=0\n",
        "bert_sentence_training_features=[]\n",
        "for i in test_data['clean_text']:\n",
        "    sys.stdout.write('\\r'+str(c))\n",
        "    bert_sentence_training_features.append(embeddToBERT(i))\n",
        "    c=c+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28df900a",
      "metadata": {
        "id": "28df900a",
        "outputId": "a85c1f61-6811-488b-9046-b7441b59462f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000, 768)\n"
          ]
        }
      ],
      "source": [
        "feature = [x.T for x in bert_sentence_training_features]\n",
        "bert_sentence_training_features = np.asarray(feature).reshape(len(test_data['clean_text']),768)\n",
        "\n",
        "print(bert_sentence_training_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7012b6a",
      "metadata": {
        "id": "b7012b6a"
      },
      "source": [
        "## Labelling test data using the model trained above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f8c362d",
      "metadata": {
        "id": "0f8c362d"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(bert_sentence_training_features)\n",
        "pred=pd.DataFrame(y_pred,dtype=int)\n",
        "pred.to_csv(\"data2/final_3_prediction_svc.dat\",index=None,header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce24ac4f",
      "metadata": {
        "id": "ce24ac4f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "ed2c89f1e5e4917409185a3e15aa5bb5780ab35aeb0c86ab70a623b6a5358c2a"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Final_3.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}